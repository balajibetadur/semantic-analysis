The article:
============

### <a href="https://medium.com/@adriensieg/text-similarities-da019229c894" class="uri">https://medium.com/@adriensieg/text-similarities-da019229c894</a>

Methods studied in the article
==============================

-   Jaccard Similarity

-   Different embeddings+ K-means

-   Different embeddings+ Cosine Similarity

-   Word2Vec + Smooth Inverse Frequency + Cosine Similarity

-   Different embeddings+LSI + Cosine Similarity

-   Different embeddings+ LDA + Jensen-Shannon distance

-   Different embeddings+ Word Mover Distance

-   Different embeddings+ Variational Auto Encoder (VAE)

-   Different embeddings+ Universal sentence encoder

-   Different embeddings+ Siamese Manhattan LSTM

-   Knowledge-based Measures

SOURCES
=======

-   **\[Incredible !!!\]** :
    <a href="https://github.com/nlptown/nlp-notebooks" class="uri">https://github.com/nlptown/nlp-notebooks</a>

    -   An Introduction to Word Embeddings
    -   Data exploration with sentence similarity
    -   Discovering and Visualizing Topics in Texts with LDA (en
        français !)
    -   Keras sentiment analysis with Elmo Embeddings
    -   Multilingual Embeddings - 1. Introduction
    -   Multilingual Embeddings - 2. Cross-lingual Sentence Similarity
    -   Multilingual Embeddings - 3. Transfer Learning
    -   NLP with pretrained models - spaCy and StanfordNLP
    -   Named Entity Recognition with Conditional Random Fields
    -   Sequence Labelling with a BiLSTM in PyTorch \[sequence labelling
        tasks such as part-of-speech tagging or named entity
        recognition\]
    -   Simple Sentence Similarity \[Word Mover’s Distance + Smooth
        Inverse Frequency + InferSent + Google Sentence Encoder +
        Pearson correlation\]
    -   Text classification with BERT in PyTorch
    -   Text classification with a CNN in PyTorch
    -   Traditional text classification with Scikit-learn \[ELI5\]
    -   Updating spaCy’s Named Entity Recognition System

-   **\[Tutorial WMD in jupyter notebook\]** :
    <a href="https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb" class="uri">https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb</a>

-   **\[Word Mover Distance\]** :
    <a href="https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector" class="uri">https://www.kaggle.com/ankitswarnkar/word-embedding-using-glove-vector</a>

-   **\[lstm-gru-sentiment-analysis\]** :
    <a href="https://github.com/javaidnabi31/Word-Embeddding-Sentiment-Classification" class="uri">https://github.com/javaidnabi31/Word-Embeddding-Sentiment-Classification</a>

<a href="https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604" class="uri">https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604</a>

-   **\[Learning Word Embedding (Mathematics)\]**
    <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html" class="uri">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a>

-   **\[A Beginner’s Aha Moments for Word2Vec\]** :
    <a href="https://yidatao.github.io/2017-08-03/word2vec-aha/" class="uri">https://yidatao.github.io/2017-08-03/word2vec-aha/</a>

-   **\[Glove, Word2Vec, Fastext classes\]** :
    <a href="https://github.com/makcedward/nlp/blob/master/sample/nlp-word_embedding.ipynb" class="uri">https://github.com/makcedward/nlp/blob/master/sample/nlp-word_embedding.ipynb</a>

-   **\[!!! Very nice tutorial about how word2vec works\]** :
    <a href="https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae" class="uri">https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae</a>

-   **\[!!! An implementation guide to Word2Vec using NumPy and Google
    Sheets\]** :
    <a href="https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281" class="uri">https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281</a>

-   **\[WordRank embedding: “crowned” is most similar to “king”, not
    word2vec’s “Canute”\]** :
    <a href="https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/" class="uri">https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/</a>

-   **\[Nice !!! \]**:
    <a href="https://github.com/JacopoMangiavacchi/SwiftNLC/tree/master/ModelNotebooks" class="uri">https://github.com/JacopoMangiavacchi/SwiftNLC/tree/master/ModelNotebooks</a>

    -   Create Model With GloVe Embedding Bidirectional With Attention
    -   Create Model With FastText Embedding
    -   Create Model With GloVe Embedding
    -   Create Model With GloVe Embedding Bidirectional
    -   Create Model With NLTK Embedding
    -   Create Model With NS Linguistic Tagger Embedding

-   **\[Tutorial from ENSAE\]** :
    <a href="http://www.xavierdupre.fr/app/papierstat/helpsphinx/notebooks/text_sentiment_wordvec.html#les-donnees" class="uri">http://www.xavierdupre.fr/app/papierstat/helpsphinx/notebooks/text_sentiment_wordvec.html#les-donnees</a>

-   **\[CoreML with GloVe Word Embedding and Recursive Neural Network -
    nice tutorial\]** :
    <a href="https://medium.com/@JMangia/coreml-with-glove-word-embedding-and-recursive-neural-network-part-2-ab238ca90970" class="uri">https://medium.com/@JMangia/coreml-with-glove-word-embedding-and-recursive-neural-network-part-2-ab238ca90970</a>

-   **\[Big Benchmark\]** :
    <a href="http://nlp.town/blog/sentence-similarity/" class="uri">http://nlp.town/blog/sentence-similarity/</a>

    -   Average W2V
    -   Average W2V + Stopwords
    -   Average W2V + TFIDF
    -   Average W2V + TFIDF + Stopwords
    -   Average Glove
    -   Average Glove + Stopwords
    -   Average Glove + TFIDF
    -   Average Glove + TFIDF + Stopwords
    -   W2V + WMD
    -   W2V + Stopwords + WMD
    -   Glove + WMD
    -   Glove + Stopwords + WMD
    -   Smooth Inverse Frequency + W2V
    -   Smooth Inverse Frequency + Glove
    -   InferSent (INF)
    -   GSE (Google Sentence Encoder)

    **InferSent (INF)** = pre-trained encoder that was developed by
    Facebook Research. It is a BiLSTM with max pooling, trained on the
    SNLI dataset, 570k English sentence pairs labelled with one of three
    categories: entailment, contradiction or neutral.

    **GSE (Google Sentence Encoder)** = Google’s answer to Facebook’s
    InferSent. It comes in two forms:
    -   an advanced model that takes the element-wise sum of the
        context-aware word representations produced by the encoding
        subgraph of a Transformer model
    -   a simpler Deep Averaging Network (DAN) where input embeddings
        for words and bigrams are averaged together and passed through a
        feed-forward deep neural network.

    ====-&gt; **work with Pearson correlation**

-   **\[How to predict Quora Question Pairs using Siamese Manhattan
    LSTM\]** :
    <a href="https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07" class="uri">https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07</a>

-   **\[Latent Semantic Indexing (LSI) - An Example with mathematics\]**
    :
    <a href="http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf" class="uri">http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf</a>

-   **\[Finding similar documents with Word2Vec and WMD\]** :
    <a href="https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html" class="uri">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a>

-   **\[Cosine Similarity\]** :
    <a href="https://www.machinelearningplus.com/nlp/cosine-similarity/" class="uri">https://www.machinelearningplus.com/nlp/cosine-similarity/</a>

-   **\[Tutorial on LSI\]** :
    <a href="http://poloclub.gatech.edu/cse6242/2018spring/slides/CSE6242-820-TextAlgorithms.pdf" class="uri">http://poloclub.gatech.edu/cse6242/2018spring/slides/CSE6242-820-TextAlgorithms.pdf</a>

<a href="http://robotics.stanford.edu/~scohen/research/emdg/emdg.html#flow_eqw_notopt" class="uri">http://robotics.stanford.edu/~scohen/research/emdg/emdg.html#flow_eqw_notopt</a>

<a href="http://robotics.stanford.edu/~rubner/slides/sld014.htm" class="uri">http://robotics.stanford.edu/~rubner/slides/sld014.htm</a>

<a href="http://jxieeducation.com/2016-06-13/Document-Similarity-With-Word-Movers-Distance/" class="uri">http://jxieeducation.com/2016-06-13/Document-Similarity-With-Word-Movers-Distance/</a>

-   **\[Beyond Cosinu &gt; Jensen-Shannon + Hypothesis Test\]** :
    <a href="http://stefansavev.com/blog/beyond-cosine-similarity/" class="uri">http://stefansavev.com/blog/beyond-cosine-similarity/</a>

-   **\[Great ressources with MANY MANY notebooks\]** :
    <a href="https://www.renom.jp/index.html?c=tutorial" class="uri">https://www.renom.jp/index.html?c=tutorial</a>

-   **\[LE TRANSPORT OPTIMAL, COUTEAU SUISSE POUR LA DATA SCIENCE\]**:
    <a href="https://weave.eu/le-transport-optimal-un-couteau-suisse-pour-la-data-science/" class="uri">https://weave.eu/le-transport-optimal-un-couteau-suisse-pour-la-data-science/</a>

-   **\[BEST TUTORIAL variational-autoencoders\]** :
    <a href="https://www.jeremyjordan.me/variational-autoencoders/" class="uri">https://www.jeremyjordan.me/variational-autoencoders/</a>

-   **\[BEST TUTORIAL : Earthmover Distance !!!!!\]** :
    <a href="https://jeremykun.com/2018/03/05/earthmover-distance/" class="uri">https://jeremykun.com/2018/03/05/earthmover-distance/</a>

Problem: Compute distance between points with uncertain locations (given
by samples, or differing observations, or clusters).

-   **\[Introduction to Wasserstein metric (earth mover’s distance)
    -&gt; Mathematics\]**:
    <a href="https://yoo2080.wordpress.com/2015/04/09/introduction-to-wasserstein-metric-earth-movers-distance/" class="uri">https://yoo2080.wordpress.com/2015/04/09/introduction-to-wasserstein-metric-earth-movers-distance/</a>

-   **\[Word Mover’s distance calculation between word pairs of two
    documents\]** :
    <a href="https://stats.stackexchange.com/questions/303050/word-movers-distance-calculation-between-word-pairs-of-two-documents" class="uri">https://stats.stackexchange.com/questions/303050/word-movers-distance-calculation-between-word-pairs-of-two-documents</a>

-   \[WMD + Word2Vec\] :
    <a href="https://github.com/stephenhky/PyWMD/blob/master/WordMoverDistanceDemo.ipynb" class="uri">https://github.com/stephenhky/PyWMD/blob/master/WordMoverDistanceDemo.ipynb</a>

-   **\[Books about Optimal Transport\]** :
    <a href="https://optimaltransport.github.io/pdf/ComputationalOT.pdf" class="uri">https://optimaltransport.github.io/pdf/ComputationalOT.pdf</a>

-   **\[NICE !!!!!! How Autoencoders work - Understanding the math and
    implementation\]** :
    <a href="https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases" class="uri">https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases</a>

-   **\[Word2Vec to convert each question into a semantic vector then
    stack a Siamese network to detect if the pair is duplicate\]** :
    <a href="http://www.erogol.com/duplicate-question-detection-deep-learning/" class="uri">http://www.erogol.com/duplicate-question-detection-deep-learning/</a>

-   \[Amazing !!!\] :
    <a href="https://github.com/makcedward/nlp" class="uri">https://github.com/makcedward/nlp</a>
    -   ***Distance Measurement:***
        -   Euclidean Distance, Cosine Similarity and Jaccard Similarity
        -   Edit Distance + Levenshtein Distance
        -   Word Moving Distance (WMD)
        -   Supervised Word Moving Distance (S-WMD)
        -   Manhattan LSTM
    -   **Text Representation:**

        **1. Traditional Method**
        -   Bag-of-words (BoW)
        -   Latent Semantic Analysis (LSA) and Latent Dirichlet
            Allocation (LDA)

        **2. Character Level**
        -   Character Embedding

        **3. Word Level**
        -   Negative Sampling and Hierarchical Softmax  
        -   Word2Vec, GloVe, fastText
        -   Contextualized Word Vectors (CoVe)
        -   Embeddings from Language Models (ELMo)
        -   Generative Pre-Training (GPT)
        -   Contextual String Embeddings
        -   Self-Governing Neural Networks (SGNN)
        -   Multi-Task Deep Neural Networks (MT-DNN)
        -   Generative Pre-Training-2 (GPT-2)
        -   Universal Language Model Fine-tuning (ULMFiT)

        **4. Sentence Level**
        -   Skip-thoughts
        -   InferSent
        -   Quick-Thoughts  
        -   General Purpose Sentence (GenSen)
        -   Bidirectional Encoder Representations from Transformers
            (BERT)

\[Zoom\] : Google Sentence Encoder
==================================

-   **\[Reference\]** :
    <a href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" class="uri">https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html</a>

-   **\[Waaaaaaaaaaaa\]**
    :<a href="https://machinelearningmastery.com/encoder-decoder-deep-learning-models-text-summarization/" class="uri">https://machinelearningmastery.com/encoder-decoder-deep-learning-models-text-summarization/</a>

![](https://github.com/adsieg/text_similarity/blob/master/pictures/sentence_embedding_micro.png)

![](https://github.com/adsieg/text_similarity/blob/master/pictures/sentence_embedding_macro.png)

![](https://github.com/adsieg/text_similarity/blob/master/pictures/google_sentence_encoder.png)
